#!/usr/bin/env python3
"""
Generate Python types from shared OpenAPI schemas.
This ensures all services use the same type definitions.
"""

import yaml
from pathlib import Path
from typing import Dict, Any, List, Set, Tuple, Optional


def parse_ref(ref: str) -> Tuple[Optional[str], str]:
    """Parse a $ref string into (module_name, type_name).
    
    Returns:
        (module_name, type_name) where module_name is None for same-file refs
    """
    if ref.startswith("./") and "#" in ref:
        # Cross-file reference: "./evaluation-status.yaml#/components/schemas/EvaluationStatus"
        file_part, path_part = ref.split("#")
        type_name = path_part.split("/")[-1]
        module_name = file_part[2:].replace(".yaml", "").replace("-", "_")
        return (module_name, type_name)
    elif ref.startswith("#/"):
        # Same-file reference: "#/components/schemas/SomeType"
        type_name = ref.split("/")[-1]
        return (None, type_name)
    else:
        # Fallback - just get the last part
        return (None, ref.split("/")[-1])


def escape_description(desc: str) -> str:
    """Escape a description string for use in Python code."""
    return desc.replace('"', '\\"')


def main():
    """Auto-discover and generate Python types from all YAML files."""
    script_dir = Path(__file__).parent
    shared_dir = script_dir.parent
    types_dir = shared_dir / "types"
    output_dir = shared_dir / "generated" / "python"
    
    # Ensure output directory exists
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Track all generated modules for __init__.py
    generated_modules = []
    all_exports = []
    
    # Auto-discover and process all YAML files
    for yaml_file in sorted(types_dir.glob("*.yaml")):
        module_name = yaml_file.stem.replace("-", "_")
        output_file = output_dir / f"{module_name}.py"
        
        print(f"Processing {yaml_file.name}...")
        exports = generate_from_yaml(yaml_file, output_file)
        
        if exports:
            generated_modules.append((module_name, exports))
            all_exports.extend(exports)
            print(f"  Generated {output_file.name} with exports: {', '.join(exports)}")
    
    # Update __init__.py with all generated types
    update_init_file(output_dir, generated_modules, all_exports)
    print(f"\nGenerated {len(generated_modules)} modules with {len(all_exports)} total exports")


def generate_from_yaml(yaml_file: Path, output_file: Path) -> List[str]:
    """Generate Python code from a single YAML file."""
    try:
        with open(yaml_file, "r") as f:
            spec = yaml.safe_load(f)
    except yaml.YAMLError as e:
        print(f"  ERROR: Invalid YAML in {yaml_file.name}: {e}")
        return []
    except IOError as e:
        print(f"  ERROR: Cannot read {yaml_file.name}: {e}")
        return []
    
    if not spec or "components" not in spec or "schemas" not in spec["components"]:
        print(f"  WARNING: No schemas found in {yaml_file.name}")
        return []
    
    schemas = spec["components"]["schemas"]
    exports = []
    
    # Generate file header
    code = generate_header(yaml_file)
    
    # Determine what imports we need
    imports = determine_imports(schemas)
    
    # Find cross-file dependencies
    cross_file_deps = find_cross_file_dependencies(schemas)
    
    code += generate_imports(imports, cross_file_deps)
    
    # Generate each schema
    for name, schema in schemas.items():
        # Skip meta-schemas or helper schemas
        if name.startswith("x-") or name == "TerminalStates":
            continue
            
        if "enum" in schema:
            code += generate_enum(name, schema)
            exports.append(name)
        elif schema.get("type") == "object":
            code += generate_model(name, schema, schemas)
            exports.append(name)
        # Skip array types and other non-object schemas
    
    # Add channel constants if this is the events file
    if yaml_file.stem == "event-contracts":
        code += generate_event_channels()
        exports.append("EventChannels")
    
    # Write the generated code
    try:
        output_file.write_text(code)
    except IOError as e:
        print(f"  ERROR: Cannot write to {output_file}: {e}")
        return []
    
    return exports


def generate_header(yaml_file: Path) -> str:
    """Generate file header with metadata."""
    return f'''"""
Generated from {yaml_file.name}
DO NOT EDIT THIS FILE DIRECTLY - Edit the source YAML instead

Generated by: shared/scripts/generate-python-types.py
Source: shared/types/{yaml_file.name}
"""

'''


def determine_imports(schemas: Dict[str, Any]) -> Set[str]:
    """Determine what imports are needed based on schemas."""
    imports = set()
    
    # Use a visitor pattern to analyze all schemas
    def visit_schema(schema: Any) -> None:
        if not isinstance(schema, dict):
            return
            
        # Check schema type and format
        schema_type = schema.get("type")
        schema_format = schema.get("format")
        
        # Datetime check
        if schema_format == "date-time":
            imports.add("datetime")
        
        # Enum check
        if "enum" in schema:
            imports.add("enum")
            
        # Pydantic model check
        if schema_type == "object" and "enum" not in schema:
            imports.add("pydantic")
            
        # Check if we need typing (for complex types)
        if schema_type in ["array", "object"] or "properties" in schema:
            imports.add("typing")
            
        # Recurse into nested schemas
        for key, value in schema.items():
            if key == "properties" and isinstance(value, dict):
                # Check for optional fields
                required = schema.get("required", [])
                if any(prop not in required for prop in value):
                    imports.add("typing")
                # Visit each property
                for prop_schema in value.values():
                    visit_schema(prop_schema)
            elif key in ["items", "additionalProperties"]:
                visit_schema(value)
            elif isinstance(value, list):
                for item in value:
                    visit_schema(item)
    
    # Visit all schemas
    for schema in schemas.values():
        visit_schema(schema)
    
    return imports


def find_cross_file_dependencies(schemas: Dict[str, Any]) -> Set[str]:
    """Find dependencies on types from other files."""
    deps = set()
    
    def visit_schema(schema: Any) -> None:
        if not isinstance(schema, dict):
            return
            
        # Check for cross-file references
        if "$ref" in schema:
            module_name, type_name = parse_ref(schema["$ref"])
            if module_name:  # Only track cross-file deps
                deps.add(f"{module_name}:{type_name}")
            return  # Don't recurse into $ref
        
        # Recurse into nested schemas
        for key, value in schema.items():
            if isinstance(value, dict):
                visit_schema(value)
            elif isinstance(value, list):
                for item in value:
                    visit_schema(item)
    
    # Visit all schemas
    for schema in schemas.values():
        visit_schema(schema)
    
    return deps


def generate_imports(imports: Set[str], cross_file_deps: Set[str]) -> str:
    """Generate import statements."""
    code = ""
    
    if "datetime" in imports:
        code += "from datetime import datetime\n"
    
    if "typing" in imports:
        typing_imports = []
        # Always include these if typing is needed
        typing_imports.extend(["Optional", "Dict", "Any", "List"])
        code += f"from typing import {', '.join(typing_imports)}\n"
    
    if "pydantic" in imports:
        code += "from pydantic import BaseModel, Field\n"
    
    if "enum" in imports:
        code += "from enum import Enum\n"
        # List is already imported in typing imports above
    
    # Import types from other files based on actual dependencies
    for dep in sorted(cross_file_deps):
        module, type_name = dep.split(":")
        code += f"from .{module} import {type_name}\n"
    
    if code:
        code += "\n"
    
    return code


def generate_enum(name: str, schema: Dict[str, Any]) -> str:
    """Generate an Enum class."""
    if "enum" not in schema or not schema["enum"]:
        print(f"  WARNING: Enum {name} has no values")
        return ""
        
    code = f"\nclass {name}(str, Enum):\n"
    
    # Add docstring
    if "description" in schema:
        code += f'    """{schema["description"]}"""\n'
    
    # Add enum values
    for value in schema["enum"]:
        # Convert to valid Python identifier
        attr_name = value.upper().replace("-", "_")
        code += f'    {attr_name} = "{value}"\n'
    
    # Add helper methods for terminal states (special case for EvaluationStatus)
    if name == "EvaluationStatus":
        code += generate_status_helpers()
    
    return code + "\n"


def generate_status_helpers() -> str:
    """Generate helper methods for EvaluationStatus."""
    return '''
    @classmethod
    def terminal_states(cls) -> List["EvaluationStatus"]:
        """Get all terminal states."""
        return [cls.COMPLETED, cls.FAILED, cls.CANCELLED]
    
    def is_terminal(self) -> bool:
        """Check if this is a terminal state."""
        return self in self.terminal_states()
    
    def is_active(self) -> bool:
        """Check if this is an active (non-terminal) state."""
        return not self.is_terminal()
'''


def generate_model(name: str, schema: Dict[str, Any], all_schemas: Dict[str, Any]) -> str:
    """Generate a Pydantic model class."""
    code = f"\nclass {name}(BaseModel):\n"
    
    # Add docstring
    if "description" in schema:
        code += f'    """{schema["description"]}"""\n'
    
    properties = schema.get("properties", {})
    required = schema.get("required", [])
    
    if not properties:
        code += "    pass\n"
        return code
    
    # Generate fields
    for prop_name, prop_def in properties.items():
        field_code = generate_field(prop_name, prop_def, prop_name in required, all_schemas)
        code += f"    {field_code}\n"
    
    return code + "\n"


def generate_field(name: str, schema: Dict[str, Any], is_required: bool, all_schemas: Dict[str, Any]) -> str:
    """Generate a single field definition."""
    # Determine the type
    field_type = determine_field_type(schema, all_schemas)
    
    # Handle optional fields
    is_nullable = schema.get("nullable", False)
    has_default = "default" in schema
    
    if not is_required or is_nullable:
        field_type = f"Optional[{field_type}]"
    
    # Build the field definition  
    if is_required and not has_default and not is_nullable:
        field_def = "Field(...)"
    else:
        default = schema.get("default")
        if default is None or is_nullable:
            field_def = "None"
        elif isinstance(default, str):
            field_def = f'"{default}"'
        elif isinstance(default, bool):
            field_def = str(default)
        elif isinstance(default, (int, float)):
            field_def = str(default)
        elif isinstance(default, dict):
            field_def = "Field(default_factory=dict)"
        elif isinstance(default, list):
            field_def = "Field(default_factory=list)"
        else:
            field_def = repr(default)
    
    # Add Field() wrapper if we have description or other metadata
    if "description" in schema:
        desc = escape_description(schema["description"])
        if field_def == "...":
            field_def = f'Field(..., description="{desc}")'
        elif field_def == "None":
            field_def = f'Field(None, description="{desc}")'
        elif not field_def.startswith("Field"):
            field_def = f'Field({field_def}, description="{desc}")'
    
    return f"{name}: {field_type} = {field_def}"


def determine_field_type(schema: Dict[str, Any], all_schemas: Dict[str, Any]) -> str:
    """Determine the Python type for a field."""
    # Handle $ref
    if "$ref" in schema:
        _, type_name = parse_ref(schema["$ref"])
        return type_name
    
    # Handle basic types
    type_map = {
        "string": "str",
        "integer": "int",
        "number": "float",
        "boolean": "bool",
        "array": "List[Any]",  # Should be more specific
        "object": "Dict[str, Any]",
    }
    
    base_type = type_map.get(schema.get("type", ""), "Any")
    
    # Handle arrays with specific item types
    if schema.get("type") == "array" and "items" in schema:
        item_type = determine_field_type(schema["items"], all_schemas)
        base_type = f"List[{item_type}]"
    
    # Handle datetime
    if schema.get("format") == "date-time":
        base_type = "datetime"
    
    return base_type


def generate_event_channels() -> str:
    """Generate event channel constants."""
    return '''
class EventChannels:
    """Redis pub/sub channel names for events."""
    EVALUATION_QUEUED = "evaluation:queued"
    EVALUATION_RUNNING = "evaluation:running"
    EVALUATION_COMPLETED = "evaluation:completed"
    EVALUATION_FAILED = "evaluation:failed"
'''


def update_init_file(output_dir: Path, modules: List[tuple], all_exports: List[str]):
    """Update __init__.py to export all generated types."""
    code = '''"""
Generated Python types from shared contracts.
DO NOT EDIT FILES IN THIS DIRECTORY - They are auto-generated.

Run 'python shared/scripts/generate-python-types.py' to regenerate.
"""

'''
    
    # Generate imports
    for module_name, exports in modules:
        if exports:
            if len(exports) == 1:
                code += f"from .{module_name} import {exports[0]}\n"
            else:
                code += f"from .{module_name} import {', '.join(exports)}\n"
    
    # Generate __all__
    code += "\n__all__ = [\n"
    for export in sorted(all_exports):
        code += f'    "{export}",\n'
    code += "]\n"
    
    init_file = output_dir / "__init__.py"
    init_file.write_text(code)
    print(f"Updated {init_file}")




if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nGeneration cancelled by user")
        exit(1)
    except Exception as e:
        print(f"\nFATAL ERROR: {e}")
        import traceback
        traceback.print_exc()
        exit(1)